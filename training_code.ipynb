{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "**Dependencies and setup**\n",
    "\n",
    "This can take a minute or so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "id": "rA38jtUgtZsG"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as tdist\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import optuna\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as disp\n",
    "from typing import Callable\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.spatial.transform import Slerp\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['PYVIRTUALDISPLAY_DISPLAYFD'] = '0'\n",
    "\n",
    "display = Display(visible=0,size=(600,600))\n",
    "display.start()\n",
    "device = T.device('cuda') if T.cuda.is_available() else T.device('cpu')\n",
    "\n",
    "video_every = 50 # take video every n episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "**Reinforcement learning agent**\n",
    "\n",
    "The agent is taken from the TD3 implemenetation of a bipedal walker solution taken from https://github.com/hmomin/TD3-Bipedal-Walker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        observationDim: int,\n",
    "        actionDim: int,\n",
    "        device: T.device,\n",
    "        size: int = 1_000_000,\n",
    "    ):\n",
    "        # use a fixed-size buffer to prevent constant list instantiations\n",
    "        self.states = T.zeros((size, observationDim), device=device)\n",
    "        self.actions = T.zeros((size, actionDim), device=device)\n",
    "        self.rewards = T.zeros(size, device=device)\n",
    "        self.nextStates = T.zeros((size, observationDim), device=device)\n",
    "        self.doneFlags = T.zeros(size, device=device)\n",
    "        self.random = np.random.default_rng()\n",
    "        # use a pointer to keep track of where in the buffer we are\n",
    "        self.pointer = 0\n",
    "        # use current size to ensure we don't train on any non-existent data points\n",
    "        self.currentSize = 0\n",
    "        self.size = size\n",
    "        self.indexes = []\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: np.ndarray,\n",
    "        reward: float,\n",
    "        nextState: np.ndarray,\n",
    "        doneFlag: bool,\n",
    "    ):\n",
    "        # store all the data for this transition\n",
    "        \n",
    "        tensorState = T.tensor(state, device=device)\n",
    "        tensorAction = T.tensor(action, device=device)\n",
    "        tensorNextState = T.tensor(nextState, device=device)\n",
    "\n",
    "        if self.currentSize < self.size:\n",
    "            self.indexes.append(self.currentSize)\n",
    "            self.currentSize += 1\n",
    "        else:\n",
    "            #To make sure all states are in order of recency\n",
    "            self.states = T.roll(self.states, shifts=-1, dims=0)\n",
    "            self.actions = T.roll(self.actions, shifts=-1, dims=0)\n",
    "            self.rewards = T.roll(self.rewards, shifts=-1, dims=0)\n",
    "            self.next_states = T.roll(self.nextStates, shifts=-1, dims=0)\n",
    "            self.dones = T.roll(self.doneFlags, shifts=-1, dims=0)\n",
    "\n",
    "        ptr = self.currentSize-1\n",
    "        self.states[ptr, :] = tensorState\n",
    "        self.actions[ptr, :] = tensorAction\n",
    "        self.rewards[ptr] = reward\n",
    "        self.nextStates[ptr, :] = tensorNextState\n",
    "        self.doneFlags[ptr] = float(doneFlag)\n",
    "\n",
    "\n",
    "    def fade(self, norm_index, fade_param): \n",
    "        return np.tanh(fade_param*norm_index**2)\n",
    "\n",
    "    def generate_probs(self, fade_param):\n",
    "        weights = 1e-7*(self.fade(np.array(self.indexes)/self.currentSize, fade_param))# weights are based solely on the history, highly squashed\n",
    "        self.probs = weights/np.sum(weights)\n",
    "        return self.probs\n",
    "\n",
    "    def getMiniBatch(self, size: int, fade_param) -> dict[str, T.Tensor]:\n",
    "        # ensure size is not bigger than the current size of the buffer\n",
    "        if size >= self.currentSize or fade_param == -1:\n",
    "            indices = T.randint(0, self.currentSize, (size,), device=device)\n",
    "        else:\n",
    "            # generate random indices\n",
    "            # indices = T.randint(0, self.currentSize, (size,), device=device)\n",
    "            indices = self.random.choice(self.indexes, p=self.generate_probs(fade_param), size=size)\n",
    "        \n",
    "        # return the mini-batch of transitions\n",
    "        return {\n",
    "            \"states\": self.states[indices, :],\n",
    "            \"actions\": self.actions[indices, :],\n",
    "            \"rewards\": self.rewards[indices],\n",
    "            \"nextStates\": self.nextStates[indices, :],\n",
    "            \"doneFlags\": self.doneFlags[indices],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        T.nn.init.xavier_uniform_(layer.weight)\n",
    "        T.nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        shape: list,\n",
    "        outputActivation: Callable,\n",
    "        learningRate: float,\n",
    "        device: T.device,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # initialize the network\n",
    "        layers = []\n",
    "        for i in range(1, len(shape)):\n",
    "            dim1 = shape[i - 1]\n",
    "            dim2 = shape[i]\n",
    "            layers.append(nn.Linear(dim1, dim2))\n",
    "            if i < len(shape) - 1:\n",
    "                layers.append(nn.ReLU())\n",
    "        layers.append(outputActivation())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learningRate)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state: T.Tensor) -> T.Tensor:\n",
    "        return self.network(state)\n",
    "\n",
    "    def gradientDescentStep(self, loss: T.Tensor, retainGraph: bool = False) -> None:\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=retainGraph)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def reinitialise(self):\n",
    "        self.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, learningRate, gamma, tau, buffer_size):\n",
    "        self.observationDim = env.observation_space.shape[0]\n",
    "        self.actionDim = env.action_space.shape[0]\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        # check if the saveFolder path exists\n",
    "\n",
    "        self.buffer = Buffer(self.observationDim, self.actionDim, device, buffer_size)\n",
    "\n",
    "        # initialize the actor and critics\n",
    "        self.actor = Network([self.observationDim, 256, 256, self.actionDim], nn.Tanh, learningRate, device)\n",
    "\n",
    "        self.critic1 =  Network([self.observationDim + self.actionDim, 256, 256, 1], nn.Identity, learningRate, device)\n",
    "\n",
    "        self.critic2 = Network([self.observationDim + self.actionDim, 256, 256, 1], nn.Identity, learningRate, device)\n",
    "        \n",
    "        # create target networks\n",
    "        self.targetActor = deepcopy(self.actor)\n",
    "        self.targetCritic1 = deepcopy(self.critic1)\n",
    "        self.targetCritic2 = deepcopy(self.critic2)\n",
    "\n",
    "    def getNoisyAction(self, state, sigma):\n",
    "        deterministicAction = self.getDeterministicAction(state)\n",
    "        noise = np.random.normal(0, sigma, deterministicAction.shape)\n",
    "        return np.clip(deterministicAction + noise, -1, +1)\n",
    "\n",
    "    def getDeterministicAction(self, state):\n",
    "        actions: T.Tensor = self.actor.forward(T.tensor(state, device=device))\n",
    "        return actions.cpu().detach().numpy()\n",
    "\n",
    "    def reset(self):\n",
    "        self.actor.reinitialise()\n",
    "        self.critic1.reinitialise()\n",
    "        self.critic2.reinitialise()\n",
    "\n",
    "    def update(self, miniBatchSize, trainingSigma, trainingClip, updatePolicy, fade_param, tau):\n",
    "        # randomly sample a mini-batch from the replay buffer\n",
    "\n",
    "        miniBatch = self.buffer.getMiniBatch(miniBatchSize, fade_param)\n",
    "        # create tensors to start generating computational graph\n",
    "        states = miniBatch[\"states\"]\n",
    "        actions = miniBatch[\"actions\"]\n",
    "        rewards = miniBatch[\"rewards\"]\n",
    "        nextStates = miniBatch[\"nextStates\"]\n",
    "        dones = miniBatch[\"doneFlags\"]\n",
    "        # compute the targets\n",
    "        targets = self.computeTargets(\n",
    "            rewards, nextStates, dones, trainingSigma, trainingClip\n",
    "        )\n",
    "        # do a single step on each critic network\n",
    "        Q1Loss = self.computeQLoss(self.critic1, states, actions, targets)\n",
    "        self.critic1.gradientDescentStep(Q1Loss, True)\n",
    "        Q2Loss = self.computeQLoss(self.critic2, states, actions, targets)\n",
    "        self.critic2.gradientDescentStep(Q2Loss)\n",
    "        if updatePolicy:\n",
    "            # do a single step on the actor network\n",
    "            policyLoss = self.computePolicyLoss(states)\n",
    "            self.actor.gradientDescentStep(policyLoss)\n",
    "            # update target networks\n",
    "            self.updateTargetNetwork(self.targetActor, self.actor, tau)\n",
    "            self.updateTargetNetwork(self.targetCritic1, self.critic1, tau)\n",
    "            self.updateTargetNetwork(self.targetCritic2, self.critic2, tau)\n",
    "\n",
    "    def computeTargets(\n",
    "        self,\n",
    "        rewards: T.Tensor,\n",
    "        nextStates: T.Tensor,\n",
    "        dones: T.Tensor,\n",
    "        trainingSigma: float,\n",
    "        trainingClip: float,\n",
    "    ) -> T.Tensor:\n",
    "        targetActions = self.targetActor.forward(nextStates.float())\n",
    "        # create additive noise for target actions\n",
    "        noise = T.normal(0, trainingSigma, targetActions.shape, device=device)\n",
    "        clippedNoise = T.clip(noise, -trainingClip, +trainingClip)\n",
    "        targetActions = T.clip(targetActions + clippedNoise, -1, +1)\n",
    "        # compute targets\n",
    "        targetQ1Values = T.squeeze(\n",
    "            self.targetCritic1.forward(T.hstack([nextStates, targetActions]).float()))\n",
    "        \n",
    "        targetQ2Values = T.squeeze(\n",
    "            self.targetCritic2.forward(T.hstack([nextStates, targetActions]).float()))\n",
    "        \n",
    "        targetQValues = T.minimum(targetQ1Values, targetQ2Values)\n",
    "        return rewards + self.gamma * (1 - dones) * targetQValues\n",
    "\n",
    "    def computeQLoss(\n",
    "        self, network: Network, states: T.Tensor, actions: T.Tensor, targets: T.Tensor\n",
    "    ) -> T.Tensor:\n",
    "        # compute the MSE of the Q function with respect to the targets\n",
    "        QValues = T.squeeze(network.forward(T.hstack([states, actions]).float()))\n",
    "        return T.square(QValues - targets).mean()\n",
    "\n",
    "    def computePolicyLoss(self, states: T.Tensor):\n",
    "        actions = self.actor.forward(states.float())\n",
    "        QValues = T.squeeze(self.critic1.forward(T.hstack([states, actions]).float()))\n",
    "        return -QValues.mean()\n",
    "\n",
    "    def updateTargetNetwork(self, targetNetwork, network, tau):\n",
    "        with T.no_grad():\n",
    "            for targetParameter, parameter in zip(targetNetwork.parameters(), network.parameters()):\n",
    "                targetParameter.mul_(1 - tau)\n",
    "                targetParameter.add_(tau * parameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineScheduler:\n",
    "    def __init__(self, first_value, end_value, step_count):\n",
    "        self.max = first_value\n",
    "        self.min = end_value\n",
    "        self.step_count = step_count\n",
    "    \n",
    "    def get_value(self, step):\n",
    "        if step >= self.step_count:\n",
    "            return self.min\n",
    "        return self.min + 0.5 * (self.max - self.min) * (1 + math.cos(step / self.step_count * math.pi))\n",
    "\n",
    "class LinearScheduler:\n",
    "    def __init__(self, first_value, end_value, step_count):\n",
    "        self.max = first_value\n",
    "        self.min = end_value\n",
    "        self.step_count = step_count\n",
    "    \n",
    "    def get_value(self, step):\n",
    "        if step >= self.step_count:\n",
    "            return self.min\n",
    "        return self.min + (self.max - self.min) * (1 - (step / self.step_count))\n",
    "\n",
    "\n",
    "class CosineSchedulerTorch:\n",
    "    def __init__(self, optimizer_list, end_value, step_count):\n",
    "        self.min = end_value\n",
    "        self.step_count = step_count\n",
    "\n",
    "        self.schedulers = []\n",
    "        for op in optimizer_list:\n",
    "            self.schedulers.append(T.optim.lr_scheduler.CosineAnnealingLR(op, step_count, end_value))\n",
    "\n",
    "    def make_step(self):\n",
    "        for op in self.schedulers:\n",
    "            op.step()\n",
    "\n",
    "\n",
    "class LinearSchedulerTorch:\n",
    "    def __init__(self, optimizer_list, end_value, step_count):\n",
    "        self.min = end_value\n",
    "        self.step_count = step_count\n",
    "\n",
    "        self.schedulers = []\n",
    "        for op in optimizer_list:\n",
    "            self.schedulers.append(T.optim.lr_scheduler.LinearLR(op, step_count, end_value))\n",
    "\n",
    "    def make_step(self):\n",
    "        for op in self.schedulers:\n",
    "            op.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select environment to be either normal or hardcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "env = gym.make(\"BipedalWalker-v3\")\n",
    "#env = gym.make(\"BipedalWalkerHardcore-v3\")\n",
    "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "**Prepare the environment and wrap it to capture videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUw4h980jfnu",
    "outputId": "29b68733-b95b-45e5-9959-761f9d38af0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has 24 observations and the agent can take 4 actions\n",
      "The device is: cuda\n",
      "It's recommended to train on the cpu for this\n"
     ]
    }
   ],
   "source": [
    "print('The environment has {} observations and the agent can take {} actions'.format(obs_dim, act_dim))\n",
    "print('The device is: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is code for normal training, it should be identical to hardcore training, in everything other than hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "rDl6ViIDlVOk",
    "outputId": "731e4ce7-c98d-4bde-8a2c-fbdf1410e24f"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "T.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "ep_reward = 0\n",
    "amended_ep_reward = 0\n",
    "reward_list = []\n",
    "plot_data = []\n",
    "learningRate = 0.000526\n",
    "tau = 0.25  # tracking parameter used to update target networks slowly\n",
    "gamma = 0.99  # discount factor for rewards\n",
    "actionSigma = 0.18  # noise in the deterministic policy output\n",
    "trainingSigma = 0.2  # noise for the target actions\n",
    "trainingClip = 0.5  # clip limit for target actions\n",
    "miniBatchSize = 100  # size of the batch size\n",
    "policyDelay = 2  # number of steps to wait before updating the policy\n",
    "min_reward = -8\n",
    "render = True\n",
    "fade_param = -1\n",
    "exploration_trials = 5\n",
    "buffer_size = 1_500_000\n",
    "# initialise agent\n",
    "agent = Agent(env, learningRate, gamma, tau, buffer_size)\n",
    "max_episodes = 3000\n",
    "max_timesteps = 2000\n",
    "plot_interval = 10 # update the plot every n episodes\n",
    "train_between_episodes = 0\n",
    "stall_reward = 60\n",
    "solution_found = False\n",
    "\n",
    "schedule_counter = 0\n",
    "schedule_period = 60\n",
    "\n",
    "scheduler_type = LinearScheduler\n",
    "\n",
    "tau_schedueler = scheduler_type(tau, 0.003, schedule_period)\n",
    "sigma_schedueler = scheduler_type(actionSigma, 0.05, schedule_period)\n",
    "train_schedueler = scheduler_type(trainingSigma, 0.125, schedule_period)\n",
    "reward_schedueler = scheduler_type(min_reward, -100, schedule_period)\n",
    "batch_schedueler = CosineScheduler(-miniBatchSize, -512, 2*schedule_period)\n",
    "lr_scheduler = CosineSchedulerTorch([agent.actor.optimizer, agent.critic1.optimizer, agent.critic2.optimizer], 0.000003, 2*schedule_period)\n",
    "\n",
    "# training procedure:\n",
    "for episode in range(1, max_episodes+1):\n",
    "    state = env.reset()\n",
    "\n",
    "    if solution_found:\n",
    "        schedule_counter += 1\n",
    "        min_reward = reward_schedueler.get_value(schedule_counter)\n",
    "        tau = tau_schedueler.get_value(schedule_counter)\n",
    "        trainingSigma = train_schedueler.get_value(schedule_counter)\n",
    "        actionSigma = sigma_schedueler.get_value(schedule_counter)\n",
    "        miniBatchSize = -int(batch_schedueler.get_value(schedule_counter))\n",
    "        lr_scheduler.make_step()\n",
    "        if schedule_counter == schedule_period:\n",
    "            fade_param = -1\n",
    "\n",
    "\n",
    "    if episode <= exploration_trials:\n",
    "        agent.reset()\n",
    "\n",
    "    for t in range(max_timesteps):\n",
    "        # select the agent action\n",
    "        action = agent.getNoisyAction(state, actionSigma)\n",
    "\n",
    "        # take action in environment and get r and s'\n",
    "        next_state, reward, done, truncated = env.step(action)\n",
    "        tuned_reward = max(reward, min_reward)\n",
    "        \n",
    "        agent.buffer.store(state, action, tuned_reward, next_state, done)\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "        amended_ep_reward += tuned_reward\n",
    "\n",
    "        shouldUpdatePolicy = t % policyDelay == 0\n",
    "        agent.update(miniBatchSize, trainingSigma, trainingClip, shouldUpdatePolicy, fade_param, tau)\n",
    "\n",
    "        # stop iterating when the episode finished\n",
    "        if done or t==(max_timesteps-1):\n",
    "            break\n",
    "\n",
    "    # append the episode reward to the reward list\n",
    "    reward_list.append(ep_reward)\n",
    "\n",
    "    if amended_ep_reward > stall_reward:\n",
    "        if not solution_found:\n",
    "            print(\"SOLUTION FOUND!!!\")\n",
    "        solution_found = True\n",
    "        \n",
    "\n",
    "    ep_reward = 0\n",
    "    amended_ep_reward = 0\n",
    "\n",
    "    if episode % plot_interval == 0:\n",
    "        plot_data.append([episode, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
    "        reward_list = []\n",
    "        plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
    "        plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
    "        plt.xlabel('Episode number')\n",
    "        plt.ylabel('Episode reward')\n",
    "        plt.show()\n",
    "        disp.clear_output(wait=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for Hardcore Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed = 42\n",
    "T.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "random.seed(seed) \n",
    "np.random.seed(seed)\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "ep_reward = 0\n",
    "amended_ep_reward = 0\n",
    "reward_list = []\n",
    "plot_data = []\n",
    "learningRate = 0.000526\n",
    "tau = 0.25  # tracking parameter used to update target networks slowly\n",
    "gamma = 0.99  # discount factor for rewards\n",
    "actionSigma = 0.18  # noise in the deterministic policy output\n",
    "trainingSigma = 0.2  # noise for the target actions\n",
    "trainingClip = 0.5  # clip limit for target actions\n",
    "miniBatchSize = 100  # size of the batch size\n",
    "policyDelay = 2  # number of steps to wait before updating the policy\n",
    "min_reward = -8\n",
    "render = True\n",
    "fade_param = 7\n",
    "exploration_trials = 5\n",
    "buffer_size = 2_000_000\n",
    "\n",
    "# initialise agent\n",
    "agent = Agent(env, learningRate, gamma, tau, buffer_size)\n",
    "max_episodes = 3000\n",
    "max_timesteps = 2000\n",
    "plot_interval = 10 # update the plot every n episodes\n",
    "train_between_episodes = 0\n",
    "stall_reward = 280\n",
    "solution_found = False\n",
    "\n",
    "schedule_counter = 0\n",
    "schedule_period = 800\n",
    "\n",
    "scheduler_type = LinearScheduler\n",
    "\n",
    "tau_schedueler = scheduler_type(tau, 0.05, schedule_period)\n",
    "sigma_schedueler = scheduler_type(actionSigma, 0.1, schedule_period)\n",
    "train_schedueler = scheduler_type(trainingSigma, 0.125, schedule_period)\n",
    "reward_schedueler = scheduler_type(min_reward, -30, schedule_period)\n",
    "batch_schedueler = scheduler_type(-miniBatchSize, -512, schedule_period)\n",
    "lr_scheduler = CosineSchedulerTorch([agent.actor.optimizer, agent.critic1.optimizer, agent.critic2.optimizer], 0.00005, schedule_period)\n",
    "\n",
    "# training procedure:\n",
    "for episode in range(1, max_episodes+1):\n",
    "    state = env.reset()\n",
    "\n",
    "    if solution_found:\n",
    "        schedule_counter += 1\n",
    "        min_reward = reward_schedueler.get_value(schedule_counter)\n",
    "        tau = tau_schedueler.get_value(schedule_counter)\n",
    "        trainingSigma = train_schedueler.get_value(schedule_counter)\n",
    "        actionSigma = sigma_schedueler.get_value(schedule_counter)\n",
    "        miniBatchSize = -int(batch_schedueler.get_value(schedule_counter))\n",
    "        lr_scheduler.make_step()\n",
    "\n",
    "    if episode <= exploration_trials:\n",
    "        agent.reset()\n",
    "\n",
    "    for t in range(max_timesteps):\n",
    "        # select the agent action\n",
    "        action = agent.getNoisyAction(state, actionSigma)\n",
    "\n",
    "        # take action in environment and get r and s'\n",
    "        next_state, reward, done, truncated = env.step(action)\n",
    "        tuned_reward = max(reward, min_reward)\n",
    "        \n",
    "        agent.buffer.store(state, action, tuned_reward, next_state, done)\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "        amended_ep_reward += tuned_reward\n",
    "\n",
    "        shouldUpdatePolicy = t % policyDelay == 0\n",
    "        agent.update(miniBatchSize, trainingSigma, trainingClip, shouldUpdatePolicy, fade_param, tau)\n",
    "\n",
    "        # stop iterating when the episode finished\n",
    "        if done or t==(max_timesteps-1):\n",
    "            break\n",
    "\n",
    "    # append the reward to a list\n",
    "    reward_list.append(ep_reward)\n",
    "\n",
    "    if amended_ep_reward > stall_reward:\n",
    "        if not solution_found:\n",
    "            print(\"SOLUTION FOUND!!!\")\n",
    "        solution_found = True\n",
    "        \n",
    "\n",
    "    ep_reward = 0\n",
    "    amended_ep_reward = 0\n",
    "\n",
    "    if episode % plot_interval == 0:\n",
    "        plot_data.append([episode, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
    "        reward_list = []\n",
    "        plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
    "        plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
    "        plt.xlabel('Episode number')\n",
    "        plt.ylabel('Episode reward')\n",
    "        plt.show()\n",
    "        disp.clear_output(wait=True)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "RL-Assignment",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
